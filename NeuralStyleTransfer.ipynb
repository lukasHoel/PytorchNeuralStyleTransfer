{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import time\n",
    "import os \n",
    "image_dir = os.getcwd() + '/Images/'\n",
    "model_dir = os.getcwd() + '/Models/'\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from PIL import Image\n",
    "from collections import OrderedDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vgg definition that conveniently let's you grab the outputs from any layer\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, pool='max'):\n",
    "        super(VGG, self).__init__()\n",
    "        #vgg modules\n",
    "        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.conv3_4 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv4_4 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv5_4 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        if pool == 'max':\n",
    "            self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            self.pool5 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        elif pool == 'avg':\n",
    "            self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "            self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "            self.pool3 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "            self.pool4 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "            self.pool5 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "            \n",
    "    def forward(self, x, out_keys):\n",
    "        out = {}\n",
    "        out['r11'] = F.relu(self.conv1_1(x))\n",
    "        out['r12'] = F.relu(self.conv1_2(out['r11']))\n",
    "        out['p1'] = self.pool1(out['r12'])\n",
    "        out['r21'] = F.relu(self.conv2_1(out['p1']))\n",
    "        out['r22'] = F.relu(self.conv2_2(out['r21']))\n",
    "        out['p2'] = self.pool2(out['r22'])\n",
    "        out['r31'] = F.relu(self.conv3_1(out['p2']))\n",
    "        out['r32'] = F.relu(self.conv3_2(out['r31']))\n",
    "        out['r33'] = F.relu(self.conv3_3(out['r32']))\n",
    "        out['r34'] = F.relu(self.conv3_4(out['r33']))\n",
    "        out['p3'] = self.pool3(out['r34'])\n",
    "        out['r41'] = F.relu(self.conv4_1(out['p3']))\n",
    "        out['r42'] = F.relu(self.conv4_2(out['r41']))\n",
    "        out['r43'] = F.relu(self.conv4_3(out['r42']))\n",
    "        out['r44'] = F.relu(self.conv4_4(out['r43']))\n",
    "        out['p4'] = self.pool4(out['r44'])\n",
    "        out['r51'] = F.relu(self.conv5_1(out['p4']))\n",
    "        out['r52'] = F.relu(self.conv5_2(out['r51']))\n",
    "        out['r53'] = F.relu(self.conv5_3(out['r52']))\n",
    "        out['r54'] = F.relu(self.conv5_4(out['r53']))\n",
    "        out['p5'] = self.pool5(out['r54'])\n",
    "        return [out[key] for key in out_keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gram matrix and loss\n",
    "class GramMatrix(nn.Module):\n",
    "    def forward(self, input):\n",
    "        b,c,h,w = input.size()\n",
    "        F = input.view(b, c, h*w)\n",
    "        G = torch.bmm(F, F.transpose(1,2)) \n",
    "        G.div_(h*w)\n",
    "        return G\n",
    "\n",
    "class GramMSELoss(nn.Module):\n",
    "    def forward(self, input, target):\n",
    "        out = nn.MSELoss()(GramMatrix()(input), target)\n",
    "        return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from torch.utils.data import Dataset, SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import os.path\n",
    "from os.path import join\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import random\n",
    "\n",
    "from typing import Union\n",
    "import cv2\n",
    "\n",
    "\n",
    "# Calculates rotation matrix to euler angles\n",
    "# The result is the same as MATLAB except the order\n",
    "# of the euler angles ( x and z are swapped ).\n",
    "def get_euler_angles(R):\n",
    "    sy = torch.sqrt(R[0,0] * R[0,0] + R[1,0] * R[1,0])\n",
    "    singular = sy < 1e-6\n",
    "\n",
    "    if not singular:\n",
    "        x = torch.atan2(R[2, 1], R[2, 2])\n",
    "        y = torch.atan2(-R[2, 0], sy)\n",
    "        z = torch.atan2(R[1, 0], R[0, 0])\n",
    "    else:\n",
    "        x = torch.atan2(-R[1, 2], R[1, 1])\n",
    "        y = torch.atan2(-R[2, 0], sy)\n",
    "        z = 0\n",
    "\n",
    "    return torch.tensor([x, y, z], dtype=x.dtype)\n",
    "\n",
    "def to_numpy(x):\n",
    "    x_ = np.array(x)\n",
    "    x_ = x_.astype(np.float32)\n",
    "    return x_\n",
    "\n",
    "def get_image_transform(transform):\n",
    "    # fix for this issue: https://github.com/pytorch/vision/issues/2194\n",
    "    if transform is not None and isinstance(transform, torchvision.transforms.Compose) and (transform.transforms[-1], torchvision.transforms.ToTensor):\n",
    "        transform = torchvision.transforms.Compose([\n",
    "            *transform.transforms[:-1],\n",
    "            torchvision.transforms.Lambda(to_numpy),\n",
    "            torchvision.transforms.ToTensor()\n",
    "        ])\n",
    "    elif isinstance(transform, torchvision.transforms.ToTensor):\n",
    "        transform = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.Lambda(to_numpy),\n",
    "            torchvision.transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    return transform\n",
    "\n",
    "\n",
    "def enet_weighing(dataloader, num_classes, c=1.02):\n",
    "    \"\"\"Computes class weights as described in the ENet paper:\n",
    "\n",
    "        w_class = 1 / (ln(c + p_class)),\n",
    "\n",
    "    where c is usually 1.02 and p_class is the propensity score of that\n",
    "    class:\n",
    "\n",
    "        propensity_score = freq_class / total_pixels.\n",
    "\n",
    "    References: https://arxiv.org/abs/1606.02147\n",
    "\n",
    "    Keyword arguments:\n",
    "    - dataloader (``data.Dataloader``): A data loader to iterate over the\n",
    "    dataset.\n",
    "    - num_classes (``int``): The number of classes.\n",
    "    - c (``int``, optional): AN additional hyper-parameter which restricts\n",
    "    the interval of values for the weights. Default: 1.02.\n",
    "\n",
    "    \"\"\"\n",
    "    class_count = 0\n",
    "    total = 0\n",
    "    print(\"Create class weights...\")\n",
    "    for batch in tqdm(dataloader):\n",
    "        label = batch[1]\n",
    "        label = label.cpu().numpy()\n",
    "\n",
    "        # Flatten label\n",
    "        flat_label = label.flatten()\n",
    "\n",
    "        # Sum up the number of pixels of each class and the total pixel\n",
    "        # counts for each label\n",
    "        class_count += np.bincount(flat_label, minlength=num_classes)\n",
    "        total += flat_label.size\n",
    "\n",
    "    # Compute propensity score and then the weights for each class\n",
    "    propensity_score = class_count / total\n",
    "\n",
    "    class_weights = 1 / (np.log(c + propensity_score))\n",
    "\n",
    "    class_weights[class_count == 0] = 0\n",
    "\n",
    "    return class_weights\n",
    "\n",
    "\n",
    "def get_color_encoding(seg_classes):\n",
    "    if seg_classes.lower() == 'nyu40':\n",
    "        \"\"\"Color palette for nyu40 labels \"\"\"\n",
    "        return OrderedDict([\n",
    "            ('void', (0, 0, 0)),\n",
    "            ('wall', (174, 199, 232)),\n",
    "            ('floor', (152, 223, 138)),\n",
    "            ('cabinet', (31, 119, 180)),\n",
    "            ('bed', (255, 187, 120)),\n",
    "            ('chair', (188, 189, 34)),\n",
    "            ('sofa', (140, 86, 75)),\n",
    "            ('table', (255, 152, 150)),\n",
    "            ('door', (214, 39, 40)),\n",
    "            ('window', (197, 176, 213)),\n",
    "            ('bookshelf', (148, 103, 189)),\n",
    "            ('picture', (196, 156, 148)),\n",
    "            ('counter', (23, 190, 207)),\n",
    "            ('blinds', (178, 76, 76)),\n",
    "            ('desk', (247, 182, 210)),\n",
    "            ('shelves', (66, 188, 102)),\n",
    "            ('curtain', (219, 219, 141)),\n",
    "            ('dresser', (140, 57, 197)),\n",
    "            ('pillow', (202, 185, 52)),\n",
    "            ('mirror', (51, 176, 203)),\n",
    "            ('floormat', (200, 54, 131)),\n",
    "            ('clothes', (92, 193, 61)),\n",
    "            ('ceiling', (78, 71, 183)),\n",
    "            ('books', (172, 114, 82)),\n",
    "            ('refrigerator', (255, 127, 14)),\n",
    "            ('television', (91, 163, 138)),\n",
    "            ('paper', (153, 98, 156)),\n",
    "            ('towel', (140, 153, 101)),\n",
    "            ('showercurtain', (158, 218, 229)),\n",
    "            ('box', (100, 125, 154)),\n",
    "            ('whiteboard', (178, 127, 135)),\n",
    "            ('person', (120, 185, 128)),\n",
    "            ('nightstand', (146, 111, 194)),\n",
    "            ('toilet', (44, 160, 44)),\n",
    "            ('sink', (112, 128, 144)),\n",
    "            ('lamp', (96, 207, 209)),\n",
    "            ('bathtub', (227, 119, 194)),\n",
    "            ('bag', (213, 92, 176)),\n",
    "            ('otherstructure', (94, 106, 211)),\n",
    "            ('otherfurniture', (82, 84, 163)),\n",
    "            ('otherprop', (100, 85, 144)),\n",
    "        ])\n",
    "    elif seg_classes.lower() == 'scannet20':\n",
    "        return OrderedDict([\n",
    "            ('unlabeled', (0, 0, 0)),\n",
    "            ('wall', (174, 199, 232)),\n",
    "            ('floor', (152, 223, 138)),\n",
    "            ('cabinet', (31, 119, 180)),\n",
    "            ('bed', (255, 187, 120)),\n",
    "            ('chair', (188, 189, 34)),\n",
    "            ('sofa', (140, 86, 75)),\n",
    "            ('table', (255, 152, 150)),\n",
    "            ('door', (214, 39, 40)),\n",
    "            ('window', (197, 176, 213)),\n",
    "            ('bookshelf', (148, 103, 189)),\n",
    "            ('picture', (196, 156, 148)),\n",
    "            ('counter', (23, 190, 207)),\n",
    "            ('desk', (247, 182, 210)),\n",
    "            ('curtain', (219, 219, 141)),\n",
    "            ('refrigerator', (255, 127, 14)),\n",
    "            ('showercurtain', (158, 218, 229)),\n",
    "            ('toilet', (44, 160, 44)),\n",
    "            ('sink', (112, 128, 144)),\n",
    "            ('bathtub', (227, 119, 194)),\n",
    "            ('otherfurniture', (82, 84, 163)),\n",
    "        ])\n",
    "\n",
    "\n",
    "class Abstract_Dataset(Dataset, ABC):\n",
    "\n",
    "    def __init__(self,\n",
    "                 root_path,\n",
    "                 transform_rgb=None,\n",
    "                 transform_label=None,\n",
    "                 transform_uv=None,\n",
    "                 crop=False,\n",
    "                 crop_size=(-1, -1),\n",
    "                 crop_random=True,\n",
    "                 resize=False,\n",
    "                 resize_size=(256, 256),\n",
    "                 create_instance_map=False,\n",
    "                 load_noise=False,\n",
    "                 load_uvs=False,\n",
    "                 load_labels=True,\n",
    "                 load_instances=True,\n",
    "                 cache=False,\n",
    "                 verbose=False):\n",
    "        # save all constructor arguments\n",
    "        self.transform_rgb = transform_rgb\n",
    "        self.transform_label = get_image_transform(transform_label)\n",
    "        self.transform_uv = transform_uv\n",
    "        self.crop = crop\n",
    "        self.crop_size = crop_size\n",
    "        self.crop_random = crop_random\n",
    "        self.resize = resize\n",
    "        self.resize_size = resize_size\n",
    "        if isinstance(resize_size, int):\n",
    "            self.resize_size = (resize_size, resize_size)\n",
    "        if isinstance(crop_size, int):\n",
    "            self.crop_size = (crop_size, crop_size)\n",
    "        self.verbose = verbose\n",
    "        self.root_path = root_path\n",
    "        self.load_uvs = load_uvs\n",
    "        self.load_labels = load_labels\n",
    "        self.load_instances = load_instances\n",
    "        self.load_noise = load_noise\n",
    "        self.create_instance_map = create_instance_map\n",
    "        self.use_cache = cache\n",
    "        self.cache = {}\n",
    "        self.label_to_color = list(get_color_encoding(\"nyu40\").items())\n",
    "        self.label_to_color = list(get_color_encoding(\"nyu40\").items())\n",
    "        self.num_classes = len(self.label_to_color)\n",
    "\n",
    "        # create data for this dataset\n",
    "        self.create_data()\n",
    "\n",
    "    def create_data(self):\n",
    "        self.rgb_images, self.label_images, self.instance_images, self.uv_maps, self.extrinsics, self.size, self.scene_dict = self.parse_scenes()\n",
    "\n",
    "        if self.create_instance_map:\n",
    "            self.instance_map, self.inverse_instance_map = self.get_instance_map()\n",
    "        else:\n",
    "            self.instance_map = None\n",
    "            self.inverse_instance_map = None\n",
    "\n",
    "    def get_class_count(self):\n",
    "        return self.num_classes\n",
    "\n",
    "    def get_instance_count(self):\n",
    "        if self.instance_map is None:\n",
    "            self.instance_map, self.inverse_instance_map = self.get_instance_map()\n",
    "\n",
    "        return len(self.instance_map.keys())\n",
    "\n",
    "    def get_instance_map(self):\n",
    "        instance_map = {}\n",
    "        inverse_instance_map = {}\n",
    "        counter = 0\n",
    "        print(\"Creating instance map...\")\n",
    "        for i in tqdm(range(self.size)):\n",
    "            item = self.__getitem__(i)\n",
    "            instance = item[2]\n",
    "            instances = [i.detach().cpu().numpy().item() for i in torch.unique(instance)]\n",
    "\n",
    "            for i in instances:\n",
    "                if i not in instance_map:\n",
    "                    instance_map[i] = {\n",
    "                        \"idx\": counter,\n",
    "                        \"priority\": 1,\n",
    "                        \"name\": counter\n",
    "                    }\n",
    "                    inverse_instance_map[counter] = {\n",
    "                        \"instance\": i,\n",
    "                        \"priority\": 1,\n",
    "                        \"name\": counter\n",
    "                    }\n",
    "                    counter += 1\n",
    "\n",
    "        return instance_map, inverse_instance_map\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_scenes(self):\n",
    "        \"\"\"\n",
    "        Return names to all scenes for the dataset.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_colors(self, scene_path):\n",
    "        \"\"\"\n",
    "        Return absolute paths to all colors images for the scene (sorted!)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_labels(self, scene_path):\n",
    "        \"\"\"\n",
    "        Return absolute paths to all label images for the scene (sorted!)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_instances(self, scene_path):\n",
    "        \"\"\"\n",
    "        Return absolute paths to all instance images for the scene (sorted!)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_extrinsics(self, scene_path):\n",
    "        \"\"\"\n",
    "        Return absolute paths to all extrinsic images for the scene (sorted!)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_uvs(self, scene_path):\n",
    "        \"\"\"\n",
    "        Return absolute paths to all uvmap images for the scene (sorted!)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def parse_scenes(self):\n",
    "        rgb_images = []\n",
    "        label_images = []\n",
    "        instance_images = []\n",
    "        uv_maps = []\n",
    "        extrinsics_matrices = []\n",
    "        scene_dict = {}\n",
    "\n",
    "        scenes = self.get_scenes()\n",
    "        if self.verbose:\n",
    "            print(\"Collecting images...\")\n",
    "            scenes = tqdm(scenes)\n",
    "\n",
    "        for scene in scenes:\n",
    "            scene_path = join(self.root_path, scene)\n",
    "            if os.path.isdir(scene_path):\n",
    "                scene_dict[scene] = {\n",
    "                    \"path\": scene_path,\n",
    "                    \"items\": 0,\n",
    "                }\n",
    "\n",
    "                colors = self.get_colors(scene_path)\n",
    "\n",
    "                if self.load_labels:\n",
    "                    labels = self.get_labels(scene_path)\n",
    "                else:\n",
    "                    labels = colors\n",
    "\n",
    "                if self.load_instances:\n",
    "                    instances = self.get_instances(scene_path)\n",
    "                else:\n",
    "                    instances = colors\n",
    "\n",
    "                extrinsics = self.get_extrinsics(scene_path)\n",
    "\n",
    "                if self.load_uvs:\n",
    "                    uvs = self.get_uvs(scene_path)\n",
    "                else:\n",
    "                    uvs = []\n",
    "\n",
    "                if len(colors) > 0 and len(colors) == len(labels) and len(labels) == len(instances) and (\n",
    "                        len(instances) == len(uvs) or not self.load_uvs) and len(instances) == len(extrinsics):\n",
    "                    rgb_images.extend(colors)\n",
    "                    label_images.extend(labels)\n",
    "                    instance_images.extend(instances)\n",
    "                    uv_maps.extend(uvs)\n",
    "                    extrinsics_matrices.extend(extrinsics)\n",
    "                    scene_dict[scene][\"items\"] = len(colors)\n",
    "                    scene_dict[scene][\"color\"] = colors\n",
    "                    scene_dict[scene][\"label\"] = labels\n",
    "                    scene_dict[scene][\"instance\"] = instances\n",
    "                    scene_dict[scene][\"extrinsics\"] = extrinsics\n",
    "\n",
    "                    if self.load_uvs:\n",
    "                        scene_dict[scene][\"uv_map\"] = uvs\n",
    "                elif self.verbose:\n",
    "                    print(f\"Scene {scene_path} rendered incomplete --> is skipped. colors: {len(colors)}, labels: {len(labels)}, instances: {len(instances)}, uvs: {len(uvs)}, extr: {len(extrinsics)}\")\n",
    "\n",
    "        assert (len(rgb_images) == len(label_images))\n",
    "        assert (len(label_images) == len(instance_images))\n",
    "        assert (len(instance_images) == len(uv_maps) or not self.load_uvs)\n",
    "        assert (len(instance_images) == len(extrinsics_matrices))\n",
    "\n",
    "        return rgb_images, label_images, instance_images, uv_maps, extrinsics_matrices, len(rgb_images), scene_dict\n",
    "\n",
    "    def get_labels_in_image(self, label_image):\n",
    "        if isinstance(label_image, torch.Tensor):\n",
    "            label_image = torchvision.transforms.ToPILImage()(label_image.cpu().int())\n",
    "\n",
    "        if not isinstance(label_image, Image.Image):\n",
    "            raise ValueError(f\"image must be of type {torch.Tensor} or {Image.Image}, but was: {label_image}\")\n",
    "\n",
    "        labels = {}\n",
    "        for i in range(label_image.size[0]):\n",
    "            for j in range(label_image.size[1]):\n",
    "                pixel = label_image.getpixel((i, j))\n",
    "                if pixel not in labels:\n",
    "                    # is ordered dict: [pixel] accesses the pixel-th item, [0] accesses the key (which is the label)\n",
    "                    labels[pixel] = self.label_to_color[pixel][0]\n",
    "\n",
    "        return labels\n",
    "\n",
    "    def get_label_masks(self, label_image):\n",
    "        if isinstance(label_image, torch.Tensor):\n",
    "            label_image = torchvision.transforms.ToPILImage()(label_image.cpu().int())\n",
    "\n",
    "        if not isinstance(label_image, Image.Image):\n",
    "            raise ValueError(f\"image must be of type {torch.Tensor} or {Image.Image}, but was: {label_image}\")\n",
    "\n",
    "        masks = {}\n",
    "        for i in range(label_image.size[0]):\n",
    "            for j in range(label_image.size[1]):\n",
    "                pixel = label_image.getpixel((i, j))\n",
    "                if pixel not in masks:\n",
    "                    masks[pixel] = np.zeros((label_image.size[1], label_image.size[0]))\n",
    "                masks[pixel][j, i] = 1\n",
    "\n",
    "        return masks\n",
    "\n",
    "    def get_instance_masks(self, instance_image, global_instance_id=True):\n",
    "        if isinstance(instance_image, torch.Tensor):\n",
    "            instance_image = torchvision.transforms.ToPILImage()(instance_image.cpu().int())\n",
    "\n",
    "        if not isinstance(instance_image, Image.Image):\n",
    "            raise ValueError(f\"image must be of type {torch.Tensor} or {Image.Image}, but was: {instance_image}\")\n",
    "\n",
    "        masks = {}\n",
    "        for i in range(instance_image.size[0]):\n",
    "            for j in range(instance_image.size[1]):\n",
    "                pixel = instance_image.getpixel((i, j))\n",
    "\n",
    "                if global_instance_id:\n",
    "                    if self.instance_map is None:\n",
    "                        raise ValueError(f\"Cannot use global_instance_id when no instance_map was created!\")\n",
    "                    pixel = self.instance_map[pixel][\"global_id\"]\n",
    "\n",
    "                if pixel not in masks:\n",
    "                    masks[pixel] = np.zeros((instance_image.size[1], instance_image.size[0]))\n",
    "                masks[pixel][j, i] = 1\n",
    "\n",
    "        return masks\n",
    "\n",
    "    def get_color_image(self, label_image):\n",
    "        if isinstance(label_image, torch.Tensor):\n",
    "            label_image = torchvision.transforms.ToPILImage()(label_image.cpu().int())\n",
    "\n",
    "        if not isinstance(label_image, Image.Image):\n",
    "            raise ValueError(f\"image must be of type {torch.Tensor} or {Image.Image}, but was: {label_image}\")\n",
    "\n",
    "        color_image = Image.new(\"RGB\", label_image.size)\n",
    "        colored_pixels = color_image.load()\n",
    "\n",
    "        for i in range(color_image.size[0]):\n",
    "            for j in range(color_image.size[1]):\n",
    "                pixel = label_image.getpixel((i, j))\n",
    "                # is ordered dict: [pixel] accesses the pixel-th item, [1] accesses the value (which is the color)\n",
    "                colored_pixels[i, j] = self.label_to_color[pixel][1]\n",
    "\n",
    "        return color_image\n",
    "\n",
    "    def get_nearest_neighbors(self, train_dataset, train_indices, test_indices, n=1, weights=[1.0, 1.0], verbose=False):\n",
    "        # get all train extrinsics\n",
    "        train_extrinsics = [train_dataset.__getitem__(i, only_extrinsics=True) for i in train_indices]\n",
    "        train_r = [get_euler_angles(r) for r in train_extrinsics]\n",
    "        train_t = [e[:3, 3] for e in train_extrinsics]\n",
    "\n",
    "        # get all test extrinsics\n",
    "        test_extrinsics = [self.__getitem__(i, only_extrinsics=True) for i in test_indices]\n",
    "        test_r = [get_euler_angles(r) for r in test_extrinsics]\n",
    "        test_t = [e[:3, 3] for e in test_extrinsics]\n",
    "        test = zip(test_indices, test_r, test_t)\n",
    "        if verbose:\n",
    "            print(f\"Calculating {n} nearest neighbors for {len(test_indices)} test images within {len(train_indices)} train images\")\n",
    "            test = tqdm(test, total=len(test_indices))\n",
    "\n",
    "        # dict of lists: i-th entry contains the \"n nearest neighbors list\" for the test index i\n",
    "        # an entry in the \"n nearest neighbors list\" has the form {\"i\": train_index, \"d\": distance to test_index}\n",
    "        neighbors = {i: [] for i in test_indices}\n",
    "\n",
    "        if n > 0:\n",
    "            for test_idx, r1, t1 in test:\n",
    "                for train_idx, r2, t2 in zip(train_indices, train_r, train_t):\n",
    "                    # calculate distance (weighted between R and T)\n",
    "                    dr = torch.sum((r2 - r1) ** 2)\n",
    "                    dt = torch.sum((t2 - t1) ** 2)\n",
    "                    d = weights[0] * dr + weights[1] * dt\n",
    "\n",
    "                    # search insertion index\n",
    "                    insert_index = 0\n",
    "                    for neighbor in neighbors[test_idx]:\n",
    "                        if neighbor[\"d\"] > d:\n",
    "                            break\n",
    "                        insert_index += 1\n",
    "\n",
    "                    # only insert if it is one of the n shortest distances\n",
    "                    if insert_index < n:\n",
    "                        # add neighbor at the correct index\n",
    "                        neighbors[test_idx].insert(insert_index, {\"i\": train_idx, \"d\": d})\n",
    "\n",
    "                        # remove neighbors that are no longer among the shortest n\n",
    "                        neighbors[test_idx] = neighbors[test_idx][:n]\n",
    "\n",
    "        return neighbors\n",
    "\n",
    "    @abstractmethod\n",
    "    def load_extrinsics(self, idx):\n",
    "        \"\"\"\n",
    "        load the extrinsics item from self.extrinsics\n",
    "\n",
    "        :param idx: the item to load\n",
    "\n",
    "        :return: the extrinsics as numpy array\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def load_uvmap(self, idx):\n",
    "        \"\"\"\n",
    "        load the uvmap item from self.uv_maps\n",
    "\n",
    "        :param idx: the item to load\n",
    "\n",
    "        :return: the uvmap as PIL image or numpy array\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def calculate_mask(self, uvmap):\n",
    "        \"\"\"\n",
    "        calculate the uvmap mask item from uvmap (valid values == 1)\n",
    "\n",
    "        :param idx: the uvmap from which to calculate the mask\n",
    "\n",
    "        :return: the mask as PIL image\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def prepare_getitem(self, idx):\n",
    "        \"\"\"\n",
    "        Implementations can prepare anything necessary for loading this idx, i.e. load a .hdf5 file\n",
    "        :param idx:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def finalize_getitem(self, idx):\n",
    "        \"\"\"\n",
    "        Implementations can finalize anything necessary after loading this idx, i.e. close a .hdf5 file\n",
    "        :param idx:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def load_rgb(self, idx):\n",
    "        return Image.open(self.rgb_images[idx])\n",
    "\n",
    "    def load_label(self, idx):\n",
    "        return Image.open(self.label_images[idx])\n",
    "\n",
    "    def load_instance(self, idx):\n",
    "        return Image.open(self.instance_images[idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, item, only_extrinsics=False):\n",
    "        if item not in self.cache:\n",
    "            self.prepare_getitem(item)\n",
    "\n",
    "            extrinsics = self.load_extrinsics(item)\n",
    "            extrinsics = torch.from_numpy(extrinsics)\n",
    "\n",
    "            if only_extrinsics:\n",
    "                self.finalize_getitem(item)\n",
    "                return extrinsics\n",
    "\n",
    "            rgb = self.load_rgb(item)\n",
    "            label = self.load_label(item)\n",
    "            instance = self.load_instance(item)\n",
    "            uv = None\n",
    "            mask = None\n",
    "\n",
    "            if self.load_uvs:\n",
    "                uv = self.load_uvmap(item)\n",
    "                mask = self.calculate_mask(uv)\n",
    "                # resize the rgb, label, instance images to the size of uv to be consistent\n",
    "                uv_size = np.asarray(uv).shape[:2]\n",
    "                rgb = torchvision.transforms.Resize(uv_size, interpolation=Image.BICUBIC)(rgb)\n",
    "                label = torchvision.transforms.Resize(uv_size, interpolation=Image.NEAREST)(label)\n",
    "                instance = torchvision.transforms.Resize(uv_size, interpolation=Image.NEAREST)(instance)\n",
    "\n",
    "            if self.crop:\n",
    "                w, h = rgb.size\n",
    "                crop_h, crop_w = self.crop_size\n",
    "                if self.crop_random:\n",
    "                    # random crop\n",
    "                    w1 = random.randint(0, w - crop_w)\n",
    "                    h1 = random.randint(0, h - crop_h)\n",
    "                else:\n",
    "                    # center crop\n",
    "                    w1 = w//2 - crop_w//2\n",
    "                    h1 = h//2 - crop_h//2\n",
    "\n",
    "                rgb = rgb.crop((w1, h1, w1 + crop_w, h1 + crop_h))\n",
    "                label = label.crop((w1, h1, w1 + crop_w, h1 + crop_h))\n",
    "                instance = instance.crop((w1, h1, w1 + crop_w, h1 + crop_h))\n",
    "\n",
    "                if self.load_uvs:\n",
    "                    if isinstance(uv, np.ndarray):\n",
    "                        uv = uv[h1:h1+crop_h, w1:w1+crop_w,:]\n",
    "                    else:\n",
    "                        uv = uv.crop((w1, h1, w1 + crop_w, h1 + crop_h))\n",
    "                    mask = mask.crop((w1, h1, w1 + crop_w, h1 + crop_h))\n",
    "\n",
    "            if self.resize and rgb.size != self.resize_size:\n",
    "                rgb = rgb.resize(self.resize_size)\n",
    "                label = label.resize(self.resize_size, Image.NEAREST)\n",
    "                instance = instance.resize(self.resize_size, Image.NEAREST)\n",
    "                if self.load_uvs:\n",
    "                    if isinstance(uv, np.ndarray):\n",
    "                        # PIL library is not able to convert a (W,H,2) image of type np.float32\n",
    "                        # for this case we use cv2 which can do it\n",
    "                        uv = cv2.resize(uv, self.resize_size, interpolation=cv2.INTER_NEAREST)\n",
    "                    else:\n",
    "                        uv = uv.resize(self.resize_size, Image.NEAREST)\n",
    "                    mask = mask.resize(self.resize_size, Image.NEAREST)\n",
    "\n",
    "            if self.transform_rgb:\n",
    "                rgb = self.transform_rgb(rgb)\n",
    "\n",
    "            if self.transform_label:\n",
    "                label = self.transform_label(label)\n",
    "                instance = self.transform_label(instance)\n",
    "\n",
    "            if isinstance(label, torch.Tensor):\n",
    "                label = label.long()\n",
    "\n",
    "            if isinstance(instance, torch.Tensor):\n",
    "                instance = instance.long()\n",
    "\n",
    "            result = (rgb, label, instance, extrinsics, item)\n",
    "\n",
    "            if self.load_uvs and self.transform_uv:\n",
    "                uv = self.transform_uv(uv)\n",
    "                mask = np.array(mask)\n",
    "                mask = self.transform_uv(mask)\n",
    "                mask = mask > -1\n",
    "                mask = mask.squeeze()  # Final shape: H x W\n",
    "                result += (uv, mask)\n",
    "\n",
    "            if self.use_cache:\n",
    "                self.cache[item] = result\n",
    "\n",
    "            self.finalize_getitem(item)\n",
    "\n",
    "            return result\n",
    "        else:\n",
    "            return self.cache[item]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from: https://github.com/krrish94/ENet-ScanNet/blob/master/data/scannet.py\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from os.path import join\n",
    "\n",
    "\n",
    "class ScanNetDataset(Abstract_Dataset):\n",
    "\n",
    "    orig_sizes = {\n",
    "        # in format (h, w)\n",
    "        \"rgb\": (240, 320),\n",
    "        \"label\": (240, 320),\n",
    "        \"uv\": (480, 640)\n",
    "    }\n",
    "\n",
    "    def __init__(self,\n",
    "                 root_path,\n",
    "                 transform_rgb=None,\n",
    "                 transform_label=None,\n",
    "                 transform_uv=None,\n",
    "                 crop=False,\n",
    "                 crop_size=(-1, -1),\n",
    "                 crop_random=True,\n",
    "                 resize=False,\n",
    "                 resize_size=(256, 256),\n",
    "                 create_instance_map=False,\n",
    "                 load_noise=False,\n",
    "                 load_uvs=False,\n",
    "                 load_stylized_images=False,\n",
    "                 cache=False,\n",
    "                 verbose=False):\n",
    "\n",
    "        self.set_stylized_image_mode(load_stylized_images)\n",
    "\n",
    "        Abstract_Dataset.__init__(self,\n",
    "                                  root_path=root_path,\n",
    "                                  transform_rgb=transform_rgb,\n",
    "                                  transform_label=transform_label,\n",
    "                                  transform_uv=transform_uv,\n",
    "                                  crop=crop,\n",
    "                                  crop_size=crop_size,\n",
    "                                  crop_random=crop_random,\n",
    "                                  resize=resize,\n",
    "                                  resize_size=resize_size,\n",
    "                                  load_noise=load_noise,\n",
    "                                  create_instance_map=create_instance_map,\n",
    "                                  load_uvs=load_uvs,\n",
    "                                  cache=cache,\n",
    "                                  verbose=verbose)\n",
    "\n",
    "    def set_stylized_image_mode(self, stylized_images: bool):\n",
    "        self.stylized_images = stylized_images\n",
    "\n",
    "    def get_scenes(self):\n",
    "        return os.listdir(self.root_path)\n",
    "\n",
    "    def get_colors(self, scene_path):\n",
    "        \"\"\"\n",
    "        Return absolute paths to all colors images for the scene (sorted!)\n",
    "        \"\"\"\n",
    "        if not self.stylized_images:\n",
    "            color_path = join(scene_path, \"color\")\n",
    "        else:\n",
    "            color_path = join(scene_path, \"stylized_color\")\n",
    "        if not os.path.exists(color_path) or not os.path.isdir(color_path):\n",
    "            return []\n",
    "\n",
    "        colors = sorted(os.listdir(color_path), key=lambda x: int(x.split(\".\")[0]))\n",
    "        colors = [join(color_path, f) for f in colors]\n",
    "\n",
    "        return colors\n",
    "\n",
    "    def get_labels(self, scene_path):\n",
    "        \"\"\"\n",
    "        Return absolute paths to all label images for the scene (sorted!)\n",
    "        \"\"\"\n",
    "        label_path = join(scene_path, \"label\")\n",
    "\n",
    "        if not os.path.exists(label_path) or not os.path.isdir(label_path):\n",
    "            return []\n",
    "\n",
    "        labels = sorted(os.listdir(label_path), key=lambda x: int(x.split(\".\")[0]))\n",
    "        labels = [join(label_path, f) for f in labels]\n",
    "\n",
    "        return labels\n",
    "\n",
    "    def get_instances(self, scene_path):\n",
    "        \"\"\"\n",
    "        Return absolute paths to all instance images for the scene (sorted!)\n",
    "        \"\"\"\n",
    "        instance_path = join(scene_path, \"instance\")\n",
    "\n",
    "        if not os.path.exists(instance_path) or not os.path.isdir(instance_path):\n",
    "            return []\n",
    "\n",
    "        instances = sorted(os.listdir(instance_path), key=lambda x: int(x.split(\".\")[0]))\n",
    "        instances = [join(instance_path, f) for f in instances]\n",
    "\n",
    "        return instances\n",
    "\n",
    "    def get_extrinsics(self, scene_path):\n",
    "        \"\"\"\n",
    "        Return absolute paths to all extrinsic images for the scene (sorted!)\n",
    "        \"\"\"\n",
    "        if self.load_noise:\n",
    "            extrinsics_path = join(scene_path, \"pose_noise\")\n",
    "        else:\n",
    "            extrinsics_path = join(scene_path, \"pose\")\n",
    "\n",
    "        if not os.path.exists(extrinsics_path) or not os.path.isdir(extrinsics_path):\n",
    "            return []\n",
    "\n",
    "        extrinsics = sorted(os.listdir(extrinsics_path), key=lambda x: int(x.split(\".\")[0]))\n",
    "        extrinsics = [join(extrinsics_path, f) for f in extrinsics]\n",
    "\n",
    "        return extrinsics\n",
    "\n",
    "    def get_uvs(self, scene_path):\n",
    "        \"\"\"\n",
    "        Return absolute paths to all uvmap images for the scene (sorted!)\n",
    "        \"\"\"\n",
    "        if self.load_noise:\n",
    "            uv_path = join(scene_path, \"uv_noise\")\n",
    "        else:\n",
    "            uv_path = join(scene_path, \"uv\")\n",
    "\n",
    "        if not os.path.exists(uv_path) or not os.path.isdir(uv_path):\n",
    "            return []\n",
    "\n",
    "        uvs = sorted(os.listdir(uv_path), key=lambda x: int(x.split(\".\")[0]))\n",
    "        uvs_npy = [join(uv_path, f) for f in uvs if \"npy\" in f]\n",
    "        uvs_png = [join(uv_path, f) for f in uvs if \"png\" in f]\n",
    "\n",
    "        if len(uvs_npy) >= len(uvs_png):\n",
    "            self.uv_npy = True\n",
    "            return uvs_npy\n",
    "        else:\n",
    "            self.uv_npy = False\n",
    "            return uvs_png\n",
    "\n",
    "    def load_extrinsics(self, idx):\n",
    "        \"\"\"\n",
    "        load the extrinsics item from self.extrinsics\n",
    "\n",
    "        :param idx: the item to load\n",
    "\n",
    "        :return: the extrinsics as numpy array\n",
    "        \"\"\"\n",
    "\n",
    "        extrinsics = open(self.extrinsics[idx], \"r\").readlines()\n",
    "        extrinsics = [[float(item) for item in line.split(\" \")] for line in extrinsics]\n",
    "        extrinsics = np.array(extrinsics, dtype=np.float32)\n",
    "\n",
    "        return extrinsics\n",
    "\n",
    "    def load_uvmap(self, idx):\n",
    "        \"\"\"\n",
    "        load the uvmap item from self.uv_maps\n",
    "\n",
    "        :param idx: the item to load\n",
    "\n",
    "        :return: the uvmap as PIL image or numpy array\n",
    "        \"\"\"\n",
    "\n",
    "        file = self.uv_maps[idx]\n",
    "        if self.uv_npy:\n",
    "            return np.load(file)\n",
    "        else:\n",
    "            return Image.open(file)\n",
    "\n",
    "    def calculate_mask(self, uvmap):\n",
    "        \"\"\"\n",
    "        calculate the uvmap mask item from uvmap (valid values == 1)\n",
    "\n",
    "        :param idx: the uvmap from which to calculate the mask\n",
    "\n",
    "        :return: the mask as PIL image\n",
    "        \"\"\"\n",
    "\n",
    "        mask = np.asarray(uvmap)\n",
    "        if self.uv_npy:\n",
    "            mask_bool = mask[:, :, 0] != 0\n",
    "            mask_bool += mask[:, :, 1] != 0\n",
    "            mask = mask_bool\n",
    "        else:\n",
    "            mask = mask[:, :, 2] == 0\n",
    "        mask = Image.fromarray(mask)\n",
    "\n",
    "        return mask\n",
    "    \n",
    "    \n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from os.path import join\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "\n",
    "class ScanNet_Single_House_Dataset(ScanNetDataset):\n",
    "\n",
    "    def __init__(self,\n",
    "                 root_path,\n",
    "                 scene=None,\n",
    "                 min_images=1000,\n",
    "                 max_images=-1,\n",
    "                 transform_rgb=None,\n",
    "                 transform_label=None,\n",
    "                 transform_uv=None,\n",
    "                 crop=False,\n",
    "                 crop_size=(-1,-1),\n",
    "                 crop_random=True,\n",
    "                 resize=False,\n",
    "                 resize_size=(256, 256),\n",
    "                 load_noise=False,\n",
    "                 load_uvs=False,\n",
    "                 load_stylized_images=False,\n",
    "                 create_instance_map=False,\n",
    "                 cache=False,\n",
    "                 verbose=False):\n",
    "\n",
    "        self.input_scene = scene\n",
    "        self.min_images = min_images\n",
    "        self.max_images = max_images\n",
    "        self.create_instance_map = create_instance_map\n",
    "\n",
    "        ScanNetDataset.__init__(self,\n",
    "                                root_path=root_path,\n",
    "                                transform_rgb=transform_rgb,\n",
    "                                transform_label=transform_label,\n",
    "                                transform_uv=transform_uv,\n",
    "                                crop=crop,\n",
    "                                crop_size=crop_size,\n",
    "                                crop_random=crop_random,\n",
    "                                resize=resize,\n",
    "                                resize_size=resize_size,\n",
    "                                load_uvs=load_uvs,\n",
    "                                load_noise=load_noise,\n",
    "                                load_stylized_images=load_stylized_images,\n",
    "                                create_instance_map=False,  # only create it afterwards when needed because we shrinken scenes anyways after this\n",
    "                                cache=cache,\n",
    "                                verbose=verbose)\n",
    "\n",
    "    def create_data(self):\n",
    "        self.scene_dict = self.parse_scenes()[-1]\n",
    "        self.rgb_images, self.label_images, self.instance_images, self.extrinsics, self.uv_maps, self.size, self.scene = self.get_scene(self.input_scene, self.min_images, self.max_images)\n",
    "\n",
    "        print(f\"Using scene: {self.scene}. Input was: {self.input_scene}\")\n",
    "\n",
    "        if self.create_instance_map:\n",
    "            self.instance_map, self.inverse_instance_map = self.get_instance_map()\n",
    "        else:\n",
    "            self.instance_map = None\n",
    "            self.inverse_instance_map = None\n",
    "\n",
    "    def get_scene(self, scene, min_images, max_images):\n",
    "        items = self.get_scene_items(scene)\n",
    "        if self.in_range(min_images, max_images, items):\n",
    "            return self.parse_scene(scene)\n",
    "        else:\n",
    "            return self.find_house(min_images, max_images)\n",
    "\n",
    "    def get_scene_items(self, scene):\n",
    "        if scene is None:\n",
    "            return None\n",
    "        elif scene not in self.scene_dict:\n",
    "            return 0\n",
    "        else:\n",
    "            return self.scene_dict[scene][\"items\"]\n",
    "\n",
    "    def in_range(self, min, max, value):\n",
    "        return (value is not None) and (min == -1 or value >= min) and (max == -1 or value <= max)\n",
    "\n",
    "    def parse_scene(self, scene):\n",
    "        h = self.scene_dict[scene]\n",
    "        if self.load_uvs:\n",
    "            return h[\"color\"], h[\"label\"], h[\"instance\"], h[\"extrinsics\"], h[\"uv_map\"], len(h[\"color\"]), scene\n",
    "        else:\n",
    "            return h[\"color\"], h[\"label\"], h[\"instance\"], h[\"extrinsics\"], [], len(h[\"color\"]), scene\n",
    "\n",
    "    def find_house(self, min_images, max_images):\n",
    "        max = -1\n",
    "        min = -1\n",
    "        scenes = self.scene_dict\n",
    "        if self.verbose:\n",
    "            scenes = tqdm(scenes)\n",
    "            print(f\"Searching for a house with more than {min_images} images\")\n",
    "        for h in scenes:\n",
    "            size = self.get_scene_items(h)\n",
    "            if max == -1 or size > max:\n",
    "                max = size\n",
    "            if min == -1 or size < min:\n",
    "                min = size\n",
    "            if self.in_range(min_images, max_images, size):\n",
    "                if self.verbose:\n",
    "                    print(f\"Using scene '{h}' which has {size} images\")\n",
    "                return self.parse_scene(h)\n",
    "        raise ValueError(f\"No scene found with {min_images} <= i <= {max_images} images. Min/Max available: {min}/{max}\")\n",
    "        \n",
    "from torchvision.transforms import ToTensor, ToPILImage, Lambda, Compose\n",
    "def add_b_channel(x):\n",
    "    \"\"\"from RG image to RGB image (b is filled with -1, i.e. the zero value in the grid range)\"\"\"\n",
    "    return torch.cat((x, torch.full_like(x[0], -1).unsqueeze(0)), dim=0)\n",
    "class Vase_Dataset(Abstract_Dataset):\n",
    "\n",
    "    orig_sizes = {\n",
    "        # in format (h, w)\n",
    "        \"rgb\": (384, 512),\n",
    "        \"uv\": (384, 512)\n",
    "    }\n",
    "\n",
    "    uv_convert = Compose([\n",
    "        ToTensor(),\n",
    "        Lambda(add_b_channel),\n",
    "        ToPILImage()\n",
    "    ])\n",
    "\n",
    "    def __init__(self,\n",
    "                 root_path,\n",
    "                 transform_rgb=None,\n",
    "                 transform_label=None,\n",
    "                 transform_uv=None,\n",
    "                 crop=False,\n",
    "                 crop_size=(-1,-1),\n",
    "                 crop_random=True,\n",
    "                 create_instance_map=False,\n",
    "                 load_noise=False,\n",
    "                 load_uvs=False,\n",
    "                 cache=False,\n",
    "                 verbose=False):\n",
    "\n",
    "        Abstract_Dataset.__init__(self,\n",
    "                                  root_path=root_path,\n",
    "                                  transform_rgb=transform_rgb,\n",
    "                                  transform_label=transform_rgb,\n",
    "                                  transform_uv=transform_uv,\n",
    "                                  crop=crop,\n",
    "                                  crop_size=crop_size,\n",
    "                                  crop_random=crop_random,\n",
    "                                  create_instance_map=False,\n",
    "                                  load_uvs=load_uvs,\n",
    "                                  load_noise=load_noise,\n",
    "                                  load_instances=False,\n",
    "                                  load_labels=False,\n",
    "                                  cache=cache,\n",
    "                                  verbose=verbose)\n",
    "\n",
    "    def get_scenes(self):\n",
    "        return [\".\"]\n",
    "\n",
    "    def get_colors(self, scene_path):\n",
    "        \"\"\"\n",
    "        Return absolute paths to all colors images for the scene (sorted!)\n",
    "        \"\"\"\n",
    "        rgb_path = join(self.root_path, \"frame\")\n",
    "        rgb_images = sorted([join(rgb_path, f) for f in os.listdir(rgb_path)])\n",
    "\n",
    "        return rgb_images\n",
    "\n",
    "    def get_labels(self, scene_path):\n",
    "        \"\"\"\n",
    "        Return absolute paths to all label images for the scene (sorted!)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_instances(self, scene_path):\n",
    "        \"\"\"\n",
    "        Return absolute paths to all instance images for the scene (sorted!)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_extrinsics(self, scene_path):\n",
    "        \"\"\"\n",
    "        Return absolute paths to all extrinsic images for the scene (sorted!)\n",
    "        \"\"\"\n",
    "        if self.load_noise:\n",
    "            raise NotImplementedError(\"noise not supported in this dataset\")\n",
    "\n",
    "        extrinsics_path = join(self.root_path, \"extrinsics\")\n",
    "        extrinsics = sorted([join(extrinsics_path, f) for f in os.listdir(extrinsics_path)])\n",
    "\n",
    "        return extrinsics\n",
    "\n",
    "    def get_uvs(self, scene_path):\n",
    "        \"\"\"\n",
    "        Return absolute paths to all uvmap images for the scene (sorted!)\n",
    "        \"\"\"\n",
    "        if self.load_noise:\n",
    "            raise NotImplementedError(\"noise not supported in this dataset\")\n",
    "\n",
    "        uv_path = join(self.root_path, \"uv\")\n",
    "        uv_maps = sorted([join(uv_path, f) for f in os.listdir(uv_path)])\n",
    "\n",
    "        return uv_maps\n",
    "\n",
    "    def load_extrinsics(self, idx):\n",
    "        \"\"\"\n",
    "        load the extrinsics item from self.extrinsics\n",
    "\n",
    "        :param idx: the item to load\n",
    "\n",
    "        :return: the extrinsics as numpy array\n",
    "        \"\"\"\n",
    "\n",
    "        # vase dataset stores only the T vector as extrinsics, so we add identity rotation...\n",
    "        T = np.load(self.extrinsics[idx])\n",
    "        extrinsics = np.eye(4, dtype=T.dtype)\n",
    "        extrinsics[:3, 3] = T\n",
    "\n",
    "        return extrinsics\n",
    "\n",
    "    def load_uvmap(self, idx):\n",
    "        \"\"\"\n",
    "        load the uvmap item from self.uv_maps\n",
    "\n",
    "        :param idx: the item to load\n",
    "\n",
    "        :return: the uvmap as PIL image or numpy array\n",
    "        \"\"\"\n",
    "        uv = np.load(self.uv_maps[idx])\n",
    "        uv = Vase_Dataset.uv_convert(uv)\n",
    "\n",
    "        return uv\n",
    "\n",
    "    def calculate_mask(self, uvmap):\n",
    "        \"\"\"\n",
    "        calculate the uvmap mask item from uvmap (valid values == 1)\n",
    "\n",
    "        :param idx: the uvmap from which to calculate the mask\n",
    "\n",
    "        :return: the mask as PIL image\n",
    "        \"\"\"\n",
    "\n",
    "        mask = np.asarray(uvmap)\n",
    "        mask = mask > -1\n",
    "        mask = np.sum(mask, axis=mask.shape.index(3))\n",
    "        mask = mask.clip(min=0, max=1)\n",
    "        mask = mask.astype(np.int32)\n",
    "\n",
    "        return Image.fromarray(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre and post processing for images\n",
    "img_size = 256\n",
    "prep = transforms.Compose([transforms.Scale(img_size),\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Lambda(lambda x: x[torch.LongTensor([2,1,0])]), #turn to BGR\n",
    "                           transforms.Normalize(mean=[0.40760392, 0.45795686, 0.48501961], #subtract imagenet mean\n",
    "                                                std=[1,1,1]),\n",
    "                           transforms.Lambda(lambda x: x.mul_(255)),\n",
    "                           transforms.Lambda(lambda x: x.cuda() if torch.cuda.is_available() else x)\n",
    "                          ])\n",
    "postpa = transforms.Compose([transforms.Lambda(lambda x: x.cpu() if torch.cuda.is_available() else x),\n",
    "                            transforms.Lambda(lambda x: x.mul_(1./255)),\n",
    "                            transforms.Normalize(mean=[-0.40760392, -0.45795686, -0.48501961], #add imagenet mean\n",
    "                                                 std=[1,1,1]),\n",
    "                            transforms.Lambda(lambda x: x[torch.LongTensor([2,1,0])]), #turn to RGB\n",
    "                           ])\n",
    "postpb = transforms.Compose([transforms.ToPILImage()])\n",
    "def postp(tensor): # to clip results in the range [0,1]\n",
    "    t = postpa(tensor)\n",
    "    #t = tensor\n",
    "    t[t>1] = 1    \n",
    "    t[t<0] = 0\n",
    "    img = postpb(t)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get network\n",
    "vgg = VGG()\n",
    "vgg.load_state_dict(torch.load(model_dir + 'vgg_conv.pth'))\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad = False\n",
    "if torch.cuda.is_available():\n",
    "    vgg.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load images, ordered as [style_image, content_image]\n",
    "img_dirs = [image_dir, image_dir]\n",
    "img_names = ['vangogh_starry_night.jpg', '0.jpg']\n",
    "imgs = [Image.open(img_dirs[i] + name) for i,name in enumerate(img_names)]\n",
    "imgs_torch = [prep(img) for img in imgs]\n",
    "if torch.cuda.is_available():\n",
    "    imgs_torch = [Variable(img.unsqueeze(0).cuda()) for img in imgs_torch]\n",
    "else:\n",
    "    imgs_torch = [Variable(img.unsqueeze(0)) for img in imgs_torch]\n",
    "style_image, content_image = imgs_torch\n",
    "\n",
    "# opt_img = Variable(torch.randn(content_image.size()).type_as(content_image.data), requires_grad=True) #random init\n",
    "opt_img = Variable(content_image.data.clone(), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display images\n",
    "for img in imgs:\n",
    "    imshow(img);show()\n",
    "    print(np.max(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "\n",
    "# init texture\n",
    "class NeuralTexture(nn.Module):\n",
    "    def __init__(self, W, H, C, random_init=False):\n",
    "        super(NeuralTexture, self).__init__()\n",
    "        self.W = W\n",
    "        self.H = H\n",
    "        self.C = C\n",
    "\n",
    "        if random_init:\n",
    "            self.data = torch.nn.Parameter(torch.rand(C, H, W, requires_grad=True), requires_grad=True)\n",
    "        else:\n",
    "            self.data = torch.nn.Parameter(torch.zeros(C, H, W, requires_grad=True), requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        y = F.grid_sample(self.data.repeat(batch_size, 1, 1, 1),\n",
    "                          x,\n",
    "                          mode='bilinear',\n",
    "                          padding_mode='border',\n",
    "                          align_corners=True) # this treats (0,0) as origin and not as the center of the lower left texel\n",
    "        return y\n",
    "\n",
    "    def get_image(self):\n",
    "        return self.data.clamp(0, 255)\n",
    "\n",
    "    def save_image(self, dir, prefix=\"\"):\n",
    "        image = to_image(self.get_image().detach().cpu())\n",
    "        file_path = join(dir, f\"{prefix}texture.jpg\")\n",
    "        image.save(file_path)\n",
    "\n",
    "    def save_texture(self, dir, prefix=\"\"):\n",
    "        image = self.get_image().detach().cpu()\n",
    "        file_path = join(dir, f\"{prefix}texture.pt\")\n",
    "        torch.save(image, file_path)\n",
    "\n",
    "\n",
    "class HierarchicalNeuralTexture(nn.Module):\n",
    "    def __init__(self, W, H, C, num_layers=4, random_init=False):\n",
    "        super(HierarchicalNeuralTexture, self).__init__()\n",
    "        self.W = W\n",
    "        self.H = H\n",
    "        self.C = C\n",
    "\n",
    "        # laplacian pyramid, i.e. first layer is (W, H), second is (W//2, H//2), third is (W//4, H//4), ...\n",
    "        self.layers = nn.ModuleList([NeuralTexture(W // pow(2, i), H // pow(2, i), C, random_init) for i in range(num_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = [layer(x) for layer in self.layers]\n",
    "        y = torch.stack(y)\n",
    "        y = torch.sum(y, dim=0)\n",
    "        y = y\n",
    "        return y\n",
    "\n",
    "    def inverse(self, uv, sampled_texture, treshold=0.1):\n",
    "        tex = []\n",
    "        mask = []\n",
    "        weight = []\n",
    "\n",
    "        for layer in self.layers:\n",
    "            t, m, w = layer.inverse(uv, sampled_texture, treshold)\n",
    "            tex.append(t)\n",
    "            mask.append(m)\n",
    "            weight.append(w)\n",
    "\n",
    "        return tex, mask, weight\n",
    "\n",
    "    def regularizer(self, weights):\n",
    "        reg = 0.0\n",
    "\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            reg += torch.mean(torch.pow(layer.data, 2.0)) * weights[i]\n",
    "\n",
    "        return reg\n",
    "\n",
    "    def get_image(self):\n",
    "        w_range = torch.arange(0, self.W, dtype=torch.float) / (self.W - 1.0) * 2.0 - 1.0\n",
    "        h_range = torch.arange(0, self.H, dtype=torch.float) / (self.H - 1.0) * 2.0 - 1.0\n",
    "\n",
    "        v, u = torch.meshgrid(h_range, w_range)\n",
    "        uv_id = torch.stack([u, v], 2)\n",
    "        uv_id = uv_id.unsqueeze(0)\n",
    "        uv_id = uv_id.type_as(self.layers[0].data)\n",
    "\n",
    "        texture = self.forward(uv_id)[0, 0:3, :, :]\n",
    "\n",
    "        return texture.clamp(0, 255)\n",
    "\n",
    "    def save_image(self, dir, prefix=\"\"):\n",
    "        texture = self.get_image()\n",
    "        texture = to_image(texture.detach().cpu())\n",
    "        file_path = join(dir, f\"{prefix}texture.jpg\")\n",
    "        texture.save(file_path)\n",
    "\n",
    "    def save_texture(self, dir, prefix=\"\"):\n",
    "        for i, l in enumerate(self.layers):\n",
    "            l.save_texture(dir, f\"{prefix}layer-{i}-\")\n",
    "\n",
    "texture = NeuralTexture(content_image.shape[-1] * 10, content_image.shape[-2] * 10, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load single uv\n",
    "uv = np.load(image_dir + \"0.npy\")\n",
    "uv_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((256, 341), interpolation=Image.NEAREST),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x * 2 - 1),  # from [0,1] to [-1,1]\n",
    "    transforms.Lambda(lambda x: x[:2]),  # delete third channel, uv is present in first two only\n",
    "    transforms.Lambda(lambda x: x.unsqueeze(0).permute(0,2,3,1)),  # from [0,1] to [-1,1]\n",
    "    transforms.Lambda(lambda x: x.cuda() if torch.cuda.is_available() else x)\n",
    "])\n",
    "uv = uv_transform(uv)\n",
    "print(uv.shape)\n",
    "print(uv.dtype)\n",
    "print(torch.min(uv), torch.max(uv))\n",
    "# cuda\n",
    "if torch.cuda.is_available():\n",
    "    uv = uv.cuda()\n",
    "    texture = texture.cuda()\n",
    "\n",
    "# load dataset\n",
    "d = Vase_Dataset(root_path=\"/home/lukas/datasets/vase\",\n",
    "                              verbose=True,\n",
    "                              transform_rgb=prep,\n",
    "                              transform_label=None,\n",
    "                              transform_uv=uv_transform,\n",
    "                              load_uvs=True,\n",
    "                              create_instance_map=False,\n",
    "                              crop=False,\n",
    "                              crop_size=256)\n",
    "\n",
    "print(d.size)\n",
    "print(d[0][5].shape)\n",
    "print(torch.min(d[0][5]))\n",
    "print(torch.max(d[0][5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load single uv\n",
    "uv = np.load(image_dir + \"0.npy\")\n",
    "uv_transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: x * 2.0 - 1.0),\n",
    "    transforms.Lambda(lambda x: cv2.resize(x, (341,256), interpolation=cv2.INTER_NEAREST)),\n",
    "    transforms.Lambda(lambda x: x[np.newaxis,...]),\n",
    "    transforms.Lambda(lambda x: torch.from_numpy(x)),\n",
    "    transforms.Lambda(lambda x: x.cuda() if torch.cuda.is_available() else x)\n",
    "])\n",
    "uv = uv_transform(uv)\n",
    "print(uv.shape)\n",
    "print(uv.dtype)\n",
    "print(torch.min(uv), torch.max(uv))\n",
    "# cuda\n",
    "if torch.cuda.is_available():\n",
    "    uv = uv.cuda()\n",
    "    texture = texture.cuda()\n",
    "\n",
    "# load dataset\n",
    "d = ScanNet_Single_House_Dataset(root_path=\"/home/lukas/datasets/ScanNet/train/images\",\n",
    "                                 scene=\"scene0230_00\",\n",
    "                              verbose=True,\n",
    "                              transform_rgb=prep,\n",
    "                              transform_label=None,\n",
    "                              transform_uv=uv_transform,\n",
    "                              load_uvs=True,\n",
    "                              create_instance_map=False,\n",
    "                              crop=False,\n",
    "                              crop_size=256,\n",
    "                              max_images=500,\n",
    "                              min_images=100)\n",
    "\n",
    "print(d.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = d[0][5].shape[2]\n",
    "h = d[0][5].shape[1]\n",
    "w_range = torch.arange(0, w, dtype=torch.float) / (w - 1.0) * 2.0 - 1.0\n",
    "h_range = torch.arange(0, h, dtype=torch.float) / (h - 1.0) * 2.0 - 1.0\n",
    "\n",
    "v, u = torch.meshgrid(h_range, w_range)\n",
    "uv_id = torch.stack([u, v], 2)\n",
    "#uv_id = uv_id[torch.randperm(uv_id.shape[0])]\n",
    "#uv_id = uv_id[:, torch.randperm(uv_id.shape[1])]\n",
    "uv_id = uv_id.type_as(d[0][5])\n",
    "uv_id = uv_id.unsqueeze(0)\n",
    "uv_id = (uv_id + 1.0) / 2.0 # only from 0..1 instead -1..1 -> smaller range!\n",
    "print(uv_id[0, 95:100, 220:225])\n",
    "\n",
    "# Rearrange batch to be the shape of [1, W * H, 2]\n",
    "uv_id_flattened = uv_id[0, 95:100, 220:225].reshape(-1, 2)\n",
    "print(uv_id_flattened.shape)\n",
    "# Compute mean and std here\n",
    "mean = uv_id_flattened.mean(0)\n",
    "std = uv_id_flattened.std(0)\n",
    "print(mean, std)\n",
    "\n",
    "\n",
    "for idx, (rgb, _, _, _, _, uvs, _) in enumerate(d):\n",
    "    print(\"ITEM: \", idx)\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        print(uvs.shape)\n",
    "        uvcount = set()\n",
    "        zeros = 0\n",
    "        for h in range(uvs.shape[1]):\n",
    "            for w in range(uvs.shape[2]):\n",
    "                uv = uvs[0, h, w]\n",
    "                uvcount.add((uv[0].cpu().numpy().item(), uv[1].cpu().numpy().item()))\n",
    "                if uv[0] == -1 and uv[1] == -1:\n",
    "                    zeros += 1\n",
    "\n",
    "        print(\"unique uvs: \", len(uvcount))\n",
    "        print(\"(0,0) uvs: \", zeros)\n",
    "        print(\"total uvs: \", uvs.shape[1] * uvs.shape[2])\n",
    "        print(\"Non-zero duplicates: \", uvs.shape[1] * uvs.shape[2] - len(uvcount) - zeros + 1)\n",
    "        print(uvs[0, 95:100, 220:225])  # this is not equal to the intensities below, so each color has more precision\n",
    "        # Rearrange batch to be the shape of [1, W * H, 2]\n",
    "        uvs_flattened = uvs[0, 95:100, 220:225].reshape(-1, 2)\n",
    "        # Compute mean and std here\n",
    "        mean = uvs_flattened.mean(0)\n",
    "        std = uvs_flattened.std(0)\n",
    "        print(mean, std)\n",
    "    \"\"\"\n",
    "    \n",
    "    # convert uv back to [0,1] and add a third dimension again\n",
    "    uvs = uvs.cpu()\n",
    "    uvs = uvs.squeeze()\n",
    "    uvs = (uvs + 1) / 2.0\n",
    "    b_channel = torch.zeros_like(uvs[:,:,0]).unsqueeze(2)\n",
    "    uvs = torch.cat((uvs, b_channel), dim=2)\n",
    "    uvs = uvs.permute(2,0,1)\n",
    "    \n",
    "    # convert uv_id back to [0,1] and add a third dimension again\n",
    "    uv_id_img = uv_id.cpu()\n",
    "    uv_id_img = uv_id_img.squeeze()\n",
    "    uv_id_img = (uv_id_img + 1) / 2.0\n",
    "    b_channel = torch.zeros_like(uv_id_img[:,:,0]).unsqueeze(2)\n",
    "    uv_id_img = torch.cat((uv_id_img, b_channel), dim=2)\n",
    "    uv_id_img = uv_id_img.permute(2,0,1)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 3)\n",
    "    ax[0].imshow(postp(rgb))\n",
    "    ax[1].imshow(torchvision.transforms.ToPILImage()(uvs))\n",
    "    ax[2].imshow(torchvision.transforms.ToPILImage()(uv_id_img))\n",
    "    plt.show()\n",
    "    \n",
    "    if idx > 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define layers, loss functions, weights and compute optimization targets\n",
    "style_layers = ['r11','r21','r31','r41', 'r51'] \n",
    "content_layers = ['r42']\n",
    "loss_layers = style_layers + content_layers\n",
    "loss_fns = [GramMSELoss()] * len(style_layers) + [nn.MSELoss()] * len(content_layers)\n",
    "if torch.cuda.is_available():\n",
    "    loss_fns = [loss_fn.cuda() for loss_fn in loss_fns]\n",
    "    \n",
    "#these are good weights settings:\n",
    "style_weights = [1e3/n**2 for n in [64,128,256,512,512]]\n",
    "content_weights = [1e0]\n",
    "weights = style_weights + content_weights\n",
    "\n",
    "#compute optimization targets\n",
    "style_targets = [GramMatrix()(A).detach() for A in vgg(style_image, style_layers)]\n",
    "content_targets = [A.detach() for A in vgg(content_image, content_layers)]\n",
    "targets = style_targets + content_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run style transfer\n",
    "max_iter = 500\n",
    "show_iter = 50\n",
    "optimizer = optim.LBFGS([opt_img]);\n",
    "n_iter=[0]\n",
    "\n",
    "while n_iter[0] <= max_iter:\n",
    "\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        out = vgg(opt_img, loss_layers)\n",
    "        layer_losses = [weights[a] * loss_fns[a](A, targets[a]) for a,A in enumerate(out)]\n",
    "        loss = sum(layer_losses)\n",
    "        loss.backward()\n",
    "        n_iter[0]+=1\n",
    "        #print loss\n",
    "        if n_iter[0]%show_iter == (show_iter-1):\n",
    "            print('Iteration: %d, loss: %f'%(n_iter[0]+1, loss.item()))\n",
    "#             print([loss_layers[li] + ': ' +  str(l.data[0]) for li,l in enumerate(layer_losses)]) #loss of each layer\n",
    "        return loss\n",
    "    \n",
    "    optimizer.step(closure)\n",
    "    \n",
    "#display result\n",
    "out_img = postp(opt_img.data[0].cpu().squeeze())\n",
    "imshow(out_img)\n",
    "gcf().set_size_inches(10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#run style transfer\n",
    "max_iter = 100000\n",
    "max_epochs = 10\n",
    "max_items_per_epoch = 160\n",
    "iter_per_item = 20\n",
    "show_iter = 500\n",
    "#optimizer = optim.LBFGS([opt_img]);\n",
    "#optimizer = optim.LBFGS(texture.parameters());\n",
    "optimizer = optim.Adam(texture.parameters(), lr=1)\n",
    "#optimizer = optim.Adam(texture.parameters(), lr=0.1)\n",
    "#scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer=optimizer, lr_lambda=lambda x: 0.1)\n",
    "n_iter=[0]\n",
    "n_epochs=[0]\n",
    "\n",
    "\n",
    "while n_epochs[0] < max_epochs:\n",
    "    #print(n_epochs[0])\n",
    "    for idx, (rgb, _, _, _, _, uv, _) in enumerate(d):\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            #out = vgg(opt_img, loss_layers)\n",
    "            out = vgg(texture(uv), loss_layers)\n",
    "            #out = vgg(texture(uv_id), loss_layers)\n",
    "            content_targets = [A.detach() for A in vgg(rgb.unsqueeze(0), content_layers)]\n",
    "            targets = style_targets + content_targets\n",
    "            layer_losses = [weights[a] * loss_fns[a](A, targets[a]) for a,A in enumerate(out)]\n",
    "            layer_losses[-1] *= 10 # weight content higher\n",
    "            loss = sum(layer_losses)\n",
    "            loss.backward()\n",
    "            n_iter[0]+=1\n",
    "            #print loss\n",
    "            if n_iter[0]%show_iter == (show_iter-1):\n",
    "                print('Iteration: %d, loss: %f'%(n_iter[0]+1, loss.item()))\n",
    "    #             print([loss_layers[li] + ': ' +  str(l.data[0]) for li,l in enumerate(layer_losses)]) #loss of each layer\n",
    "            return loss\n",
    "\n",
    "        iterations = 0\n",
    "        while iterations < iter_per_item and n_iter[0] <= max_iter:\n",
    "            optimizer.step(closure)\n",
    "            iterations += 1\n",
    "        \n",
    "        if n_iter[0] > max_iter:\n",
    "            break\n",
    "        if idx >= max_items_per_epoch:\n",
    "            break\n",
    "    n_epochs[0]+=1\n",
    "    #scheduler.step()\n",
    "    \n",
    "#display result\n",
    "#out_img = postp(opt_img.data[0].cpu().squeeze())\n",
    "out_img = postp(texture(uv)[0].cpu())\n",
    "#out_img = postp(texture(uv_id)[0].cpu())\n",
    "imshow(out_img)\n",
    "gcf().set_size_inches(10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (rgb, _, _, _, _, uv, _) in enumerate(d):\n",
    "    out_img = postp(texture(uv)[0].cpu())\n",
    "    imshow(out_img);show()\n",
    "    gcf().set_size_inches(10,10)\n",
    "    if idx >= max_items_per_epoch:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# load noisy dataset\n",
    "d_noise = ScanNet_Single_House_Dataset(root_path=\"/home/lukas/datasets/ScanNet/train/images\",\n",
    "                                 scene=\"scene0230_00\",\n",
    "                              verbose=True,\n",
    "                              transform_rgb=prep,\n",
    "                              transform_label=None,\n",
    "                              transform_uv=uv_transform,\n",
    "                              load_uvs=True,\n",
    "                              create_instance_map=False,\n",
    "                              crop=False,\n",
    "                              load_noise=True,\n",
    "                              crop_size=256,\n",
    "                              max_images=500,\n",
    "                              min_images=100)\n",
    "\n",
    "# create video of all items in the dataset\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "video = cv2.VideoWriter(\"/home/lukas/Desktop/gatys_texture_noise.mp4\", fourcc, 20.0, (341,256))\n",
    "\n",
    "for idx, (rgb, _, _, _, _, uv, _) in enumerate(d_noise):\n",
    "    out_img = postp(texture(uv)[0].cpu())\n",
    "    out_img = np.asarray(out_img)\n",
    "    out_img = cv2.cvtColor(out_img, cv2.COLOR_RGB2BGR)\n",
    "    video.write(out_img)\n",
    "    \n",
    "cv2.destroyAllWindows()\n",
    "video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_img = postp(texture.get_image().cpu())\n",
    "out_img = postp(texture(uv_id)[0].cpu())\n",
    "imshow(out_img)\n",
    "gcf().set_size_inches(10,10)\n",
    "\n",
    "texture_img = texture.get_image().cpu()\n",
    "texture_img = postp(texture_img)\n",
    "texture_img.save(\"/home/lukas/Desktop/texture.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make the image high-resolution as described in \n",
    "#\"Controlling Perceptual Factors in Neural Style Transfer\", Gatys et al. \n",
    "#(https://arxiv.org/abs/1611.07865)\n",
    "\n",
    "#hr preprocessing\n",
    "img_size_hr = 800 #works for 8GB GPU, make larger if you have 12GB or more\n",
    "prep_hr = transforms.Compose([transforms.Scale(img_size_hr),\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Lambda(lambda x: x[torch.LongTensor([2,1,0])]), #turn to BGR\n",
    "                           transforms.Normalize(mean=[0.40760392, 0.45795686, 0.48501961], #subtract imagenet mean\n",
    "                                                std=[1,1,1]),\n",
    "                           transforms.Lambda(lambda x: x.mul_(255)),\n",
    "                          ])\n",
    "#prep hr images\n",
    "imgs_torch = [prep_hr(img) for img in imgs]\n",
    "if torch.cuda.is_available():\n",
    "    imgs_torch = [Variable(img.unsqueeze(0).cuda()) for img in imgs_torch]\n",
    "else:\n",
    "    imgs_torch = [Variable(img.unsqueeze(0)) for img in imgs_torch]\n",
    "style_image, content_image = imgs_torch\n",
    "\n",
    "#now initialise with upsampled lowres result\n",
    "opt_img = prep_hr(out_img).unsqueeze(0)\n",
    "opt_img = Variable(opt_img.type_as(content_image.data), requires_grad=True)\n",
    "\n",
    "#compute hr targets\n",
    "style_targets = [GramMatrix()(A).detach() for A in vgg(style_image, style_layers)]\n",
    "content_targets = [A.detach() for A in vgg(content_image, content_layers)]\n",
    "targets = style_targets + content_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run style transfer for high res \n",
    "max_iter_hr = 200\n",
    "optimizer = optim.LBFGS([opt_img]);\n",
    "n_iter=[0]\n",
    "while n_iter[0] <= max_iter_hr:\n",
    "\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        out = vgg(opt_img, loss_layers)\n",
    "        layer_losses = [weights[a] * loss_fns[a](A, targets[a]) for a,A in enumerate(out)]\n",
    "        loss = sum(layer_losses)\n",
    "        loss.backward()\n",
    "        n_iter[0]+=1\n",
    "        #print loss\n",
    "        if n_iter[0]%show_iter == (show_iter-1):\n",
    "            print('Iteration: %d, loss: %f'%(n_iter[0]+1, loss.item()))\n",
    "#             print([loss_layers[li] + ': ' +  str(l.data[0]) for li,l in enumerate(layer_losses)]) #loss of each layer\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)\n",
    "    \n",
    "#display result\n",
    "out_img_hr = postp(opt_img.data[0].cpu().squeeze())\n",
    "imshow(out_img_hr)\n",
    "gcf().set_size_inches(10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nvs",
   "language": "python",
   "name": "nvs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
